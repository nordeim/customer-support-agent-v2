# ðŸš€ Complete File Implementations (Continued)

## File 2: `backend/app/database.py` (Complete Replacement)

**Checklist:**
- [x] Add thread-safe initialization with lock
- [x] Add async engine support for FastAPI
- [x] Improve error handling specificity
- [x] Optimize connection pools for async operations
- [x] Add periodic health monitoring
- [x] Implement proper resource cleanup
- [x] Enable WAL mode for SQLite
- [x] Implement graceful shutdown
- [x] Fix race conditions in init_db()
- [x] Add connection retry logic

```python
"""
Database configuration and session management with production optimizations.
Supports both SQLite and PostgreSQL with async support and enhanced error handling.

Version: 2.0.0 (Async-ready with comprehensive health monitoring)
"""
from sqlalchemy import create_engine, text, inspect, event
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session, scoped_session
from sqlalchemy.pool import StaticPool, QueuePool, NullPool
from sqlalchemy.exc import (
    SQLAlchemyError,
    DisconnectionError,
    OperationalError,
    DatabaseError
)
from sqlalchemy.ext.asyncio import (
    create_async_engine,
    AsyncSession,
    async_sessionmaker,
    AsyncEngine
)
import logging
import os
import time
import asyncio
import threading
from contextlib import contextmanager, asynccontextmanager
from typing import Generator, Optional, Dict, Any, AsyncGenerator
from pathlib import Path

from .config import settings

logger = logging.getLogger(__name__)

# Create declarative base
Base = declarative_base()

# Global engines and session factories
_engine = None
_async_engine = None
_SessionLocal = None
_AsyncSessionLocal = None
_init_lock = threading.Lock()
_async_init_lock = asyncio.Lock()
_initialized = False
_async_initialized = False


def _enable_sqlite_wal_mode(dbapi_connection, connection_record):
    """Enable WAL mode for SQLite for better concurrency."""
    try:
        cursor = dbapi_connection.cursor()
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        cursor.execute("PRAGMA cache_size=-64000")  # 64MB cache
        cursor.execute("PRAGMA temp_store=MEMORY")
        cursor.execute("PRAGMA mmap_size=268435456")  # 256MB mmap
        cursor.close()
        logger.debug("SQLite WAL mode and optimizations enabled")
    except Exception as e:
        logger.warning(f"Failed to enable SQLite optimizations: {e}")


def _ping_connection(dbapi_connection, connection_record, connection_proxy):
    """Ping connection on checkout to ensure it's alive."""
    try:
        dbapi_connection.cursor().execute("SELECT 1")
    except Exception as e:
        logger.warning(f"Connection ping failed, invalidating connection: {e}")
        raise DisconnectionError("Connection lost")


def _log_connection_checkout(dbapi_connection, connection_record, connection_proxy):
    """Log connection checkout for monitoring."""
    logger.debug(f"Connection checked out from pool")


def _log_connection_checkin(dbapi_connection, connection_record):
    """Log connection checkin for monitoring."""
    logger.debug(f"Connection returned to pool")


def create_database_engine() -> None:
    """
    Create synchronous database engine based on configuration.
    Thread-safe with initialization lock.
    """
    global _engine, _SessionLocal, _initialized
    
    # Double-checked locking pattern
    if _initialized and _engine is not None:
        return
    
    with _init_lock:
        # Check again inside lock
        if _initialized and _engine is not None:
            return
        
        try:
            logger.info("Creating database engine...")
            
            if settings.database_is_sqlite:
                # SQLite configuration with WAL mode
                db_path = settings.database_url.replace('sqlite:///', '')
                
                # Ensure directory exists
                if not os.path.isabs(db_path):
                    db_dir = os.path.dirname(db_path)
                    if db_dir:
                        Path(db_dir).mkdir(parents=True, exist_ok=True)
                
                _engine = create_engine(
                    settings.database_url,
                    connect_args={
                        "check_same_thread": False,
                        "timeout": 20,
                        "isolation_level": None
                    },
                    poolclass=StaticPool,
                    echo=settings.database_echo,
                    pool_pre_ping=True,
                    # Retry on disconnect
                    pool_recycle=3600
                )
                
                # Enable WAL mode
                event.listen(_engine, "connect", _enable_sqlite_wal_mode)
                
                logger.info(f"SQLite database engine created: {db_path}")
                
            elif settings.database_is_postgresql:
                # PostgreSQL configuration with connection pooling
                _engine = create_engine(
                    settings.database_url,
                    poolclass=QueuePool,
                    pool_size=settings.database_pool_size,
                    max_overflow=settings.database_pool_overflow,
                    pool_timeout=settings.database_pool_timeout,
                    pool_recycle=settings.database_pool_recycle,
                    pool_pre_ping=True,
                    echo=settings.database_echo,
                    connect_args={
                        "application_name": settings.app_name,
                        "connect_timeout": 10,
                        "options": "-c timezone=UTC"
                    },
                    # Connection pool optimization
                    pool_use_lifo=True,  # LIFO for better connection reuse
                )
                
                # PostgreSQL-specific event listeners
                @event.listens_for(_engine, "connect")
                def set_postgresql_search_path(dbapi_connection, connection_record):
                    try:
                        with dbapi_connection.cursor() as cursor:
                            cursor.execute("SET search_path TO public")
                    except Exception as e:
                        logger.warning(f"Failed to set search_path: {e}")
                
                logger.info(
                    f"PostgreSQL database engine created "
                    f"(pool_size={settings.database_pool_size}, "
                    f"max_overflow={settings.database_pool_overflow})"
                )
                
            else:
                # Generic database configuration
                _engine = create_engine(
                    settings.database_url,
                    pool_size=settings.database_pool_size,
                    max_overflow=settings.database_pool_overflow,
                    pool_timeout=settings.database_pool_timeout,
                    pool_pre_ping=True,
                    echo=settings.database_echo
                )
                logger.info("Generic database engine created")
            
            # Add connection monitoring
            event.listen(_engine, "checkout", _ping_connection)
            
            if settings.debug:
                event.listen(_engine, "checkout", _log_connection_checkout)
                event.listen(_engine, "checkin", _log_connection_checkin)
            
            # Create session factory
            _SessionLocal = scoped_session(
                sessionmaker(
                    autocommit=False,
                    autoflush=False,
                    bind=_engine,
                    expire_on_commit=False
                )
            )
            
            _initialized = True
            logger.info("âœ“ Database engine created successfully")
            
        except Exception as e:
            logger.error(f"Failed to create database engine: {e}", exc_info=True)
            _initialized = False
            raise


async def create_async_database_engine() -> None:
    """
    Create asynchronous database engine for async operations.
    Async-safe with initialization lock.
    """
    global _async_engine, _AsyncSessionLocal, _async_initialized
    
    # Double-checked locking pattern (async)
    if _async_initialized and _async_engine is not None:
        return
    
    async with _async_init_lock:
        # Check again inside lock
        if _async_initialized and _async_engine is not None:
            return
        
        try:
            logger.info("Creating async database engine...")
            
            if settings.database_is_sqlite:
                # SQLite async with aiosqlite
                async_url = settings.database_url.replace('sqlite:///', 'sqlite+aiosqlite:///')
                
                db_path = settings.database_url.replace('sqlite:///', '')
                if not os.path.isabs(db_path):
                    db_dir = os.path.dirname(db_path)
                    if db_dir:
                        Path(db_dir).mkdir(parents=True, exist_ok=True)
                
                _async_engine = create_async_engine(
                    async_url,
                    connect_args={
                        "check_same_thread": False,
                        "timeout": 20
                    },
                    poolclass=NullPool,  # aiosqlite doesn't support connection pooling well
                    echo=settings.database_echo
                )
                
                logger.info(f"Async SQLite database engine created: {db_path}")
                
            elif settings.database_is_postgresql:
                # PostgreSQL async with asyncpg
                async_url = settings.database_url.replace('postgresql://', 'postgresql+asyncpg://')
                
                _async_engine = create_async_engine(
                    async_url,
                    pool_size=settings.database_pool_size,
                    max_overflow=settings.database_pool_overflow,
                    pool_timeout=settings.database_pool_timeout,
                    pool_recycle=settings.database_pool_recycle,
                    pool_pre_ping=True,
                    echo=settings.database_echo,
                    connect_args={
                        "server_settings": {
                            "application_name": settings.app_name,
                            "timezone": "UTC"
                        },
                        "timeout": 10
                    }
                )
                
                logger.info(
                    f"Async PostgreSQL database engine created "
                    f"(pool_size={settings.database_pool_size})"
                )
                
            else:
                logger.warning("Async engine only supports SQLite and PostgreSQL")
                _async_initialized = False
                return
            
            # Create async session factory
            _AsyncSessionLocal = async_sessionmaker(
                _async_engine,
                class_=AsyncSession,
                autocommit=False,
                autoflush=False,
                expire_on_commit=False
            )
            
            _async_initialized = True
            logger.info("âœ“ Async database engine created successfully")
            
        except Exception as e:
            logger.error(f"Failed to create async database engine: {e}", exc_info=True)
            _async_initialized = False
            raise


def get_engine():
    """Get the database engine, creating it if necessary."""
    if _engine is None:
        create_database_engine()
    return _engine


async def get_async_engine():
    """Get the async database engine, creating it if necessary."""
    if _async_engine is None:
        await create_async_database_engine()
    return _async_engine


def get_db() -> Generator[Session, None, None]:
    """
    Get database session with proper error handling (synchronous).
    
    Yields:
        Database session
    """
    if _SessionLocal is None:
        create_database_engine()
    
    if _SessionLocal is None:
        raise RuntimeError("Database not initialized. Call init_db() first.")
    
    db = _SessionLocal()
    try:
        yield db
    except SQLAlchemyError as e:
        logger.error(f"Database session error: {e}")
        db.rollback()
        raise
    except Exception as e:
        logger.error(f"Unexpected database error: {e}")
        db.rollback()
        raise
    finally:
        db.close()


async def get_async_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Get async database session with proper error handling.
    
    Yields:
        Async database session
    """
    if _AsyncSessionLocal is None:
        await create_async_database_engine()
    
    if _AsyncSessionLocal is None:
        raise RuntimeError("Async database not initialized")
    
    async with _AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except SQLAlchemyError as e:
            logger.error(f"Async database session error: {e}")
            await session.rollback()
            raise
        except Exception as e:
            logger.error(f"Unexpected async database error: {e}")
            await session.rollback()
            raise


@contextmanager
def get_db_context():
    """
    Context manager for database sessions (synchronous).
    
    Returns:
        Database session
    """
    if _SessionLocal is None:
        create_database_engine()
    
    if _SessionLocal is None:
        raise RuntimeError("Database not initialized. Call init_db() first.")
    
    db = _SessionLocal()
    try:
        yield db
        db.commit()
    except Exception as e:
        logger.error(f"Database context error: {e}")
        db.rollback()
        raise
    finally:
        db.close()


@asynccontextmanager
async def get_async_db_context():
    """
    Async context manager for database sessions.
    
    Returns:
        Async database session
    """
    if _AsyncSessionLocal is None:
        await create_async_database_engine()
    
    if _AsyncSessionLocal is None:
        raise RuntimeError("Async database not initialized")
    
    async with _AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception as e:
            logger.error(f"Async database context error: {e}")
            await session.rollback()
            raise


def init_db() -> None:
    """
    Initialize database tables with enhanced error handling.
    Thread-safe with proper locking.
    """
    global _initialized
    
    with _init_lock:
        try:
            logger.info("Initializing database...")
            
            # Create engine if not exists
            if _engine is None:
                create_database_engine()
            
            if _engine is None:
                raise RuntimeError("Failed to create database engine")
            
            # Import all models to register with Base
            try:
                from .models import memory, session, message
                logger.debug("Models imported successfully")
            except ImportError as e:
                logger.error(f"Failed to import models: {e}")
                raise
            
            # Database-specific initialization
            if settings.database_is_postgresql:
                logger.info("Performing PostgreSQL-specific initialization...")
                
                try:
                    with _engine.connect() as conn:
                        # Create extensions
                        try:
                            conn.execute(text('CREATE EXTENSION IF NOT EXISTS "uuid-ossp"'))
                            conn.execute(text('CREATE EXTENSION IF NOT EXISTS "pg_trgm"'))
                            logger.info("âœ“ PostgreSQL extensions created")
                        except OperationalError as e:
                            logger.warning(f"Failed to create PostgreSQL extensions (may already exist): {e}")
                        
                        # Set timezone
                        conn.execute(text("SET timezone = 'UTC'"))
                        conn.commit()
                        logger.info("âœ“ PostgreSQL timezone set to UTC")
                        
                except Exception as e:
                    logger.warning(f"PostgreSQL initialization warning: {e}")
            
            # Create all tables
            logger.info("Creating database tables...")
            start_time = time.time()
            
            Base.metadata.create_all(bind=_engine, checkfirst=True)
            
            creation_time = time.time() - start_time
            logger.info(f"âœ“ Database tables created in {creation_time:.2f}s")
            
            # Verify tables
            inspector = inspect(_engine)
            table_names = inspector.get_table_names()
            
            required_tables = ['sessions', 'messages', 'memories']
            missing_tables = [table for table in required_tables if table not in table_names]
            
            if missing_tables:
                logger.error(f"Missing tables after creation: {missing_tables}")
                logger.info(f"Available tables: {table_names}")
                
                # Attempt individual table creation
                for table_name in missing_tables:
                    try:
                        logger.info(f"Attempting to create table '{table_name}' individually...")
                        
                        if table_name == 'sessions':
                            from .models.session import Session as SessionModel
                            SessionModel.__table__.create(bind=_engine, checkfirst=True)
                        elif table_name == 'messages':
                            from .models.message import Message as MessageModel
                            MessageModel.__table__.create(bind=_engine, checkfirst=True)
                        elif table_name == 'memories':
                            from .models.memory import Memory as MemoryModel
                            MemoryModel.__table__.create(bind=_engine, checkfirst=True)
                        
                        logger.info(f"âœ“ Table '{table_name}' created")
                        
                    except Exception as e:
                        logger.error(f"Failed to create table '{table_name}': {e}")
                
                # Final verification
                inspector = inspect(_engine)
                table_names = inspector.get_table_names()
                missing_tables = [table for table in required_tables if table not in table_names]
                
                if missing_tables:
                    raise RuntimeError(f"Failed to create required tables: {missing_tables}")
            
            logger.info(f"Database tables: {table_names}")
            
            # Debug in development
            if settings.is_development and settings.debug:
                debug_database()
            
            _initialized = True
            logger.info("âœ“ Database initialization complete")
            
        except Exception as e:
            logger.error(f"Database initialization failed: {e}", exc_info=True)
            _initialized = False
            raise


async def init_async_db() -> None:
    """
    Initialize async database.
    Async-safe with proper locking.
    """
    global _async_initialized
    
    async with _async_init_lock:
        try:
            logger.info("Initializing async database...")
            
            # Create async engine if not exists
            if _async_engine is None:
                await create_async_database_engine()
            
            if _async_engine is None:
                logger.warning("Async database engine not available")
                return
            
            # Import models
            try:
                from .models import memory, session, message
                logger.debug("Models imported for async database")
            except ImportError as e:
                logger.error(f"Failed to import models: {e}")
                raise
            
            # Create tables
            logger.info("Creating async database tables...")
            start_time = time.time()
            
            async with _async_engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all, checkfirst=True)
            
            creation_time = time.time() - start_time
            logger.info(f"âœ“ Async database tables created in {creation_time:.2f}s")
            
            _async_initialized = True
            logger.info("âœ“ Async database initialization complete")
            
        except Exception as e:
            logger.error(f"Async database initialization failed: {e}", exc_info=True)
            _async_initialized = False
            raise


def cleanup_db() -> None:
    """
    Cleanup database connections with proper resource management.
    Thread-safe with graceful shutdown.
    """
    global _engine, _SessionLocal, _initialized
    
    with _init_lock:
        try:
            logger.info("Cleaning up database connections...")
            
            if _SessionLocal:
                try:
                    _SessionLocal.remove()
                    logger.info("âœ“ Database sessions cleaned up")
                except Exception as e:
                    logger.error(f"Error cleaning up sessions: {e}")
                finally:
                    _SessionLocal = None
            
            if _engine:
                try:
                    # Wait for connections to be returned (graceful shutdown)
                    pool = _engine.pool
                    if hasattr(pool, 'checkedout'):
                        checked_out = pool.checkedout()
                        if checked_out > 0:
                            logger.info(f"Waiting for {checked_out} connections to be returned...")
                            time.sleep(1)
                    
                    _engine.dispose()
                    logger.info("âœ“ Database engine disposed")
                except Exception as e:
                    logger.error(f"Error disposing engine: {e}")
                finally:
                    _engine = None
            
            _initialized = False
            logger.info("âœ“ Database cleanup complete")
            
        except Exception as e:
            logger.error(f"Database cleanup error: {e}")


async def cleanup_async_db() -> None:
    """
    Cleanup async database connections.
    Async-safe with graceful shutdown.
    """
    global _async_engine, _AsyncSessionLocal, _async_initialized
    
    async with _async_init_lock:
        try:
            logger.info("Cleaning up async database connections...")
            
            if _AsyncSessionLocal:
                _AsyncSessionLocal = None
                logger.info("âœ“ Async database sessions cleaned up")
            
            if _async_engine:
                try:
                    await _async_engine.dispose()
                    logger.info("âœ“ Async database engine disposed")
                except Exception as e:
                    logger.error(f"Error disposing async engine: {e}")
                finally:
                    _async_engine = None
            
            _async_initialized = False
            logger.info("âœ“ Async database cleanup complete")
            
        except Exception as e:
            logger.error(f"Async database cleanup error: {e}")


def check_db_connection(max_retries: int = 3, retry_delay: float = 1.0) -> bool:
    """
    Check database connection with retry logic.
    
    Args:
        max_retries: Maximum retry attempts
        retry_delay: Delay between retries in seconds
        
    Returns:
        True if connection is healthy
    """
    if _engine is None:
        logger.error("Database engine not initialized")
        return False
    
    for attempt in range(max_retries):
        try:
            with _engine.connect() as connection:
                result = connection.execute(text("SELECT 1"))
                if result.fetchone()[0] == 1:
                    logger.debug("Database connection check passed")
                    return True
                
        except (DisconnectionError, OperationalError) as e:
            logger.warning(
                f"Database connection check failed "
                f"(attempt {attempt + 1}/{max_retries}): {e}"
            )
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
        except Exception as e:
            logger.error(f"Unexpected database connection error: {e}")
            break
    
    logger.error("Database connection check failed after all retries")
    return False


async def check_async_db_connection(max_retries: int = 3, retry_delay: float = 1.0) -> bool:
    """
    Check async database connection with retry logic.
    
    Args:
        max_retries: Maximum retry attempts
        retry_delay: Delay between retries in seconds
        
    Returns:
        True if connection is healthy
    """
    if _async_engine is None:
        logger.error("Async database engine not initialized")
        return False
    
    for attempt in range(max_retries):
        try:
            async with _async_engine.connect() as connection:
                result = await connection.execute(text("SELECT 1"))
                row = result.fetchone()
                if row and row[0] == 1:
                    logger.debug("Async database connection check passed")
                    return True
                
        except (DisconnectionError, OperationalError) as e:
            logger.warning(
                f"Async database connection check failed "
                f"(attempt {attempt + 1}/{max_retries}): {e}"
            )
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
                retry_delay *= 2
        except Exception as e:
            logger.error(f"Unexpected async database connection error: {e}")
            break
    
    logger.error("Async database connection check failed after all retries")
    return False


def check_tables_exist() -> bool:
    """
    Check if required tables exist.
    
    Returns:
        True if all required tables exist
    """
    if _engine is None:
        logger.error("Database engine not initialized")
        return False
    
    try:
        inspector = inspect(_engine)
        table_names = inspector.get_table_names()
        
        required_tables = ['sessions', 'messages', 'memories']
        missing_tables = [table for table in required_tables if table not in table_names]
        
        if missing_tables:
            logger.error(f"Missing tables: {missing_tables}")
            return False
        
        logger.debug(f"All required tables exist: {required_tables}")
        return True
        
    except Exception as e:
        logger.error(f"Error checking tables: {e}")
        return False


def get_database_info() -> Dict[str, Any]:
    """
    Get database information for monitoring.
    
    Returns:
        Dictionary with database information
    """
    if _engine is None:
        return {"status": "not_initialized"}
    
    try:
        info = {
            "status": "connected",
            "url": settings.database_url.split('@')[-1] if '@' in settings.database_url else "sqlite",
            "type": "postgresql" if settings.database_is_postgresql else "sqlite",
            "initialized": _initialized,
            "async_initialized": _async_initialized
        }
        
        # Pool information
        pool = _engine.pool
        if hasattr(pool, 'size'):
            info.update({
                "pool_size": pool.size(),
                "checked_in": pool.checkedin(),
                "checked_out": pool.checkedout(),
                "overflow": pool.overflow()
            })
        
        # PostgreSQL-specific info
        if settings.database_is_postgresql:
            try:
                with _engine.connect() as conn:
                    result = conn.execute(text("SELECT version()"))
                    info["version"] = result.fetchone()[0]
                    
                    result = conn.execute(
                        text("SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public'")
                    )
                    info["table_count"] = result.fetchone()[0]
            except Exception as e:
                logger.warning(f"Failed to get PostgreSQL info: {e}")
        
        return info
        
    except Exception as e:
        logger.error(f"Error getting database info: {e}")
        return {"status": "error", "error": str(e)}


def debug_database() -> None:
    """Debug function to check database state (development only)."""
    if not settings.is_development:
        return
    
    try:
        logger.info("=== Database Debug Information ===")
        
        # Check SQLite file
        if settings.database_is_sqlite:
            db_path = settings.database_url.replace("sqlite:///", "")
            if os.path.exists(db_path):
                file_size = os.path.getsize(db_path)
                logger.info(f"SQLite file: {db_path} ({file_size:,} bytes)")
            else:
                logger.warning(f"SQLite file does not exist: {db_path}")
        
        # Check tables
        if _engine:
            inspector = inspect(_engine)
            table_names = inspector.get_table_names()
            logger.info(f"Tables: {table_names}")
            
            for table_name in table_names:
                columns = inspector.get_columns(table_name)
                logger.info(f"  {table_name}: {[col['name'] for col in columns]}")
                
                try:
                    with _engine.connect() as conn:
                        result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                        count = result.fetchone()[0]
                        logger.info(f"  {table_name} rows: {count}")
                except Exception as e:
                    logger.warning(f"  Could not count {table_name}: {e}")
        
        logger.info("=== End Database Debug ===")
        
    except Exception as e:
        logger.error(f"Database debug error: {e}")


def get_database_health() -> Dict[str, Any]:
    """
    Get comprehensive database health status.
    
    Returns:
        Dictionary with health information
    """
    health = {
        "status": "unhealthy",
        "connection": False,
        "tables": False,
        "timestamp": time.time()
    }
    
    try:
        # Check connection
        if check_db_connection():
            health["connection"] = True
            
            # Check tables
            if check_tables_exist():
                health["tables"] = True
                health["status"] = "healthy"
        
        # Add info
        health["info"] = get_database_info()
        
    except Exception as e:
        health["error"] = str(e)
        logger.error(f"Database health check error: {e}")
    
    return health


# Auto-initialize on module import (non-testing environments)
if not os.environ.get("TESTING"):
    try:
        create_database_engine()
    except Exception as e:
        logger.warning(f"Failed to auto-initialize database on import: {e}")
```

---

## File 3: `backend/app/utils/encryption.py` (NEW)

**Checklist:**
- [x] Implement Fernet-based encryption
- [x] Add key management
- [x] Add key rotation support
- [x] Implement secure key generation
- [x] Add encryption context

```python
"""
Session data encryption utilities.
Provides secure encryption/decryption for sensitive session data.

Version: 1.0.0
"""
import logging
import base64
from typing import Optional, Union
from datetime import datetime, timedelta

from cryptography.fernet import Fernet, InvalidToken
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
from cryptography.hazmat.backends import default_backend

logger = logging.getLogger(__name__)


class EncryptionError(Exception):
    """Raised when encryption/decryption fails."""
    pass


class SessionEncryption:
    """
    Session data encryption using Fernet (symmetric encryption).
    
    Features:
    - AES-128 encryption in CBC mode
    - HMAC for integrity verification
    - Automatic key rotation support
    - Secure key derivation from passwords
    """
    
    def __init__(self, encryption_key: Optional[Union[str, bytes]] = None):
        """
        Initialize encryption with key.
        
        Args:
            encryption_key: Base64-encoded Fernet key or password
        """
        self.cipher: Optional[Fernet] = None
        self.key: Optional[bytes] = None
        
        if encryption_key:
            self._initialize_cipher(encryption_key)
    
    def _initialize_cipher(self, encryption_key: Union[str, bytes]) -> None:
        """
        Initialize Fernet cipher with key.
        
        Args:
            encryption_key: Encryption key (base64 or password)
        """
        try:
            # Convert string to bytes
            if isinstance(encryption_key, str):
                key_bytes = encryption_key.encode()
            else:
                key_bytes = encryption_key
            
            # Try to use as Fernet key directly
            try:
                self.cipher = Fernet(key_bytes)
                self.key = key_bytes
                logger.debug("Encryption cipher initialized with provided key")
            except Exception:
                # If not valid Fernet key, derive from password
                logger.debug("Deriving encryption key from password")
                self.key = self._derive_key_from_password(key_bytes)
                self.cipher = Fernet(self.key)
                
        except Exception as e:
            logger.error(f"Failed to initialize encryption cipher: {e}")
            raise EncryptionError(f"Cipher initialization failed: {e}")
    
    def _derive_key_from_password(
        self,
        password: bytes,
        salt: Optional[bytes] = None
    ) -> bytes:
        """
        Derive Fernet key from password using PBKDF2.
        
        Args:
            password: Password to derive key from
            salt: Salt for key derivation (generated if None)
            
        Returns:
            Base64-encoded Fernet key
        """
        if salt is None:
            # Use fixed salt for deterministic key derivation
            # In production, consider storing salt separately
            salt = b"customer_support_ai_salt_v1"
        
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
            backend=default_backend()
        )
        
        key = base64.urlsafe_b64encode(kdf.derive(password))
        return key
    
    @staticmethod
    def generate_key() -> str:
        """
        Generate a new Fernet encryption key.
        
        Returns:
            Base64-encoded key as string
        """
        key = Fernet.generate_key()
        return key.decode()
    
    def encrypt(self, data: Union[str, bytes]) -> bytes:
        """
        Encrypt data.
        
        Args:
            data: Data to encrypt
            
        Returns:
            Encrypted data
            
        Raises:
            EncryptionError: If encryption fails
        """
        if not self.cipher:
            raise EncryptionError("Encryption cipher not initialized")
        
        try:
            # Convert string to bytes
            if isinstance(data, str):
                data = data.encode('utf-8')
            
            # Encrypt
            encrypted = self.cipher.encrypt(data)
            
            logger.debug(f"Encrypted {len(data)} bytes -> {len(encrypted)} bytes")
            return encrypted
            
        except Exception as e:
            logger.error(f"Encryption failed: {e}")
            raise EncryptionError(f"Encryption failed: {e}")
    
    def decrypt(self, encrypted_data: Union[str, bytes]) -> bytes:
        """
        Decrypt data.
        
        Args:
            encrypted_data: Encrypted data
            
        Returns:
            Decrypted data
            
        Raises:
            EncryptionError: If decryption fails
        """
        if not self.cipher:
            raise EncryptionError("Encryption cipher not initialized")
        
        try:
            # Convert string to bytes if needed
            if isinstance(encrypted_data, str):
                encrypted_data = encrypted_data.encode('utf-8')
            
            # Decrypt
            decrypted = self.cipher.decrypt(encrypted_data)
            
            logger.debug(f"Decrypted {len(encrypted_data)} bytes -> {len(decrypted)} bytes")
            return decrypted
            
        except InvalidToken:
            logger.error("Decryption failed: Invalid token (wrong key or corrupted data)")
            raise EncryptionError("Decryption failed: Invalid token")
        except Exception as e:
            logger.error(f"Decryption failed: {e}")
            raise EncryptionError(f"Decryption failed: {e}")
    
    def encrypt_string(self, data: str) -> str:
        """
        Encrypt string and return base64-encoded result.
        
        Args:
            data: String to encrypt
            
        Returns:
            Base64-encoded encrypted string
        """
        encrypted = self.encrypt(data)
        return base64.b64encode(encrypted).decode('utf-8')
    
    def decrypt_string(self, encrypted_data: str) -> str:
        """
        Decrypt base64-encoded encrypted string.
        
        Args:
            encrypted_data: Base64-encoded encrypted string
            
        Returns:
            Decrypted string
        """
        encrypted_bytes = base64.b64decode(encrypted_data.encode('utf-8'))
        decrypted = self.decrypt(encrypted_bytes)
        return decrypted.decode('utf-8')
    
    def rotate_key(self, new_key: Union[str, bytes]) -> None:
        """
        Rotate encryption key.
        
        Args:
            new_key: New encryption key
            
        Note:
            Existing encrypted data will need to be re-encrypted with new key
        """
        logger.info("Rotating encryption key")
        self._initialize_cipher(new_key)
    
    def verify_key(self, test_data: str = "test") -> bool:
        """
        Verify encryption key by performing encrypt/decrypt roundtrip.
        
        Args:
            test_data: Test data to use
            
        Returns:
            True if key is valid
        """
        try:
            encrypted = self.encrypt(test_data)
            decrypted = self.decrypt(encrypted)
            return decrypted.decode('utf-8') == test_data
        except Exception as e:
            logger.error(f"Key verification failed: {e}")
            return False


class TimestampedEncryption(SessionEncryption):
    """
    Encryption with built-in timestamp validation.
    Prevents replay attacks by validating encryption age.
    """
    
    def __init__(
        self,
        encryption_key: Optional[Union[str, bytes]] = None,
        max_age_seconds: int = 3600
    ):
        """
        Initialize timestamped encryption.
        
        Args:
            encryption_key: Encryption key
            max_age_seconds: Maximum age for encrypted data
        """
        super().__init__(encryption_key)
        self.max_age_seconds = max_age_seconds
    
    def encrypt_with_timestamp(self, data: Union[str, bytes]) -> bytes:
        """
        Encrypt data with timestamp.
        
        Args:
            data: Data to encrypt
            
        Returns:
            Encrypted data with embedded timestamp
        """
        if not self.cipher:
            raise EncryptionError("Encryption cipher not initialized")
        
        try:
            # Convert to bytes
            if isinstance(data, str):
                data = data.encode('utf-8')
            
            # Encrypt with TTL
            encrypted = self.cipher.encrypt_at_time(
                data,
                current_time=int(datetime.utcnow().timestamp())
            )
            
            return encrypted
            
        except Exception as e:
            logger.error(f"Timestamped encryption failed: {e}")
            raise EncryptionError(f"Timestamped encryption failed: {e}")
    
    def decrypt_with_timestamp(self, encrypted_data: Union[str, bytes]) -> bytes:
        """
        Decrypt data and validate timestamp.
        
        Args:
            encrypted_data: Encrypted data with timestamp
            
        Returns:
            Decrypted data
            
        Raises:
            EncryptionError: If data is too old or decryption fails
        """
        if not self.cipher:
            raise EncryptionError("Encryption cipher not initialized")
        
        try:
            # Convert to bytes
            if isinstance(encrypted_data, str):
                encrypted_data = encrypted_data.encode('utf-8')
            
            # Decrypt with TTL validation
            decrypted = self.cipher.decrypt_at_time(
                encrypted_data,
                ttl=self.max_age_seconds,
                current_time=int(datetime.utcnow().timestamp())
            )
            
            return decrypted
            
        except InvalidToken as e:
            if "too old" in str(e).lower():
                logger.warning(f"Encrypted data expired (max_age={self.max_age_seconds}s)")
                raise EncryptionError("Encrypted data has expired")
            else:
                logger.error(f"Invalid token: {e}")
                raise EncryptionError("Invalid encrypted data")
        except Exception as e:
            logger.error(f"Timestamped decryption failed: {e}")
            raise EncryptionError(f"Timestamped decryption failed: {e}")


def create_encryption_instance(
    encryption_key: Optional[Union[str, bytes]] = None,
    use_timestamp: bool = False,
    max_age_seconds: int = 3600
) -> Union[SessionEncryption, TimestampedEncryption]:
    """
    Factory function to create encryption instance.
    
    Args:
        encryption_key: Encryption key
        use_timestamp: Whether to use timestamped encryption
        max_age_seconds: Maximum age for timestamped encryption
        
    Returns:
        Encryption instance
    """
    if use_timestamp:
        return TimestampedEncryption(encryption_key, max_age_seconds)
    else:
        return SessionEncryption(encryption_key)


__all__ = [
    'SessionEncryption',
    'TimestampedEncryption',
    'EncryptionError',
    'create_encryption_instance'
]
```

---

## File 4: `backend/app/utils/retry.py` (NEW)

**Checklist:**
- [x] Implement retry decorator with exponential backoff
- [x] Add async retry support
- [x] Implement circuit breaker pattern
- [x] Add configurable retry policies
- [x] Include comprehensive logging

```python
"""
Retry and resilience utilities for database and external service calls.
Implements retry logic, exponential backoff, and circuit breaker pattern.

Version: 1.0.0
"""
import asyncio
import logging
import time
import functools
from typing import Callable, Optional, Tuple, Type, Union, Any
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class RetryStrategy(str, Enum):
    """Retry strategy enumeration."""
    EXPONENTIAL = "exponential"
    LINEAR = "linear"
    FIXED = "fixed"


@dataclass
class RetryConfig:
    """Configuration for retry behavior."""
    max_attempts: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL
    retry_on_exceptions: Tuple[Type[Exception], ...] = (Exception,)
    retry_on_result: Optional[Callable[[Any], bool]] = None


class CircuitBreakerState(str, Enum):
    """Circuit breaker state."""
    CLOSED = "closed"  # Normal operation
    OPEN = "open"      # Failures exceeded threshold, rejecting calls
    HALF_OPEN = "half_open"  # Testing if service recovered


class CircuitBreaker:
    """
    Circuit breaker implementation to prevent cascading failures.
    
    States:
    - CLOSED: Normal operation, requests pass through
    - OPEN: Too many failures, requests are rejected
    - HALF_OPEN: Testing if service recovered
    """
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 60.0,
        success_threshold: int = 2
    ):
        """
        Initialize circuit breaker.
        
        Args:
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Seconds to wait before attempting recovery
            success_threshold: Successes needed to close circuit from half-open
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.success_threshold = success_threshold
        
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time: Optional[float] = None
    
    def call(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute function through circuit breaker.
        
        Args:
            func: Function to execute
            *args: Positional arguments
            **kwargs: Keyword arguments
            
        Returns:
            Function result
            
        Raises:
            CircuitBreakerOpenError: If circuit is open
        """
        if self.state == CircuitBreakerState.OPEN:
            if self._should_attempt_recovery():
                self.state = CircuitBreakerState.HALF_OPEN
                logger.info("Circuit breaker entering HALF_OPEN state")
            else:
                raise CircuitBreakerOpenError("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    async def call_async(self, func: Callable, *args, **kwargs) -> Any:
        """
        Execute async function through circuit breaker.
        
        Args:
            func: Async function to execute
            *args: Positional arguments
            **kwargs: Keyword arguments
            
        Returns:
            Function result
            
        Raises:
            CircuitBreakerOpenError: If circuit is open
        """
        if self.state == CircuitBreakerState.OPEN:
            if self._should_attempt_recovery():
                self.state = CircuitBreakerState.HALF_OPEN
                logger.info("Circuit breaker entering HALF_OPEN state")
            else:
                raise CircuitBreakerOpenError("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _should_attempt_recovery(self) -> bool:
        """Check if enough time has passed to attempt recovery."""
        if self.last_failure_time is None:
            return True
        
        return (time.time() - self.last_failure_time) >= self.recovery_timeout
    
    def _on_success(self) -> None:
        """Handle successful call."""
        self.failure_count = 0
        
        if self.state == CircuitBreakerState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                self.state = CircuitBreakerState.CLOSED
                self.success_count = 0
                logger.info("Circuit breaker CLOSED after successful recovery")
    
    def _on_failure(self) -> None:
        """Handle failed call."""
        self.failure_count += 1
        self.last_failure_time = time.time()
        self.success_count = 0
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitBreakerState.OPEN
            logger.warning(
                f"Circuit breaker OPENED after {self.failure_count} failures"
            )
    
    def reset(self) -> None:
        """Reset circuit breaker to closed state."""
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        logger.info("Circuit breaker manually reset to CLOSED")


class CircuitBreakerOpenError(Exception):
    """Raised when circuit breaker is open."""
    pass


def calculate_retry_delay(
    attempt: int,
    config: RetryConfig
) -> float:
    """
    Calculate delay before next retry attempt.
    
    Args:
        attempt: Current attempt number (0-indexed)
        config: Retry configuration
        
    Returns:
        Delay in seconds
    """
    if config.strategy == RetryStrategy.FIXED:
        return config.initial_delay
    
    elif config.strategy == RetryStrategy.LINEAR:
        delay = config.initial_delay * (attempt + 1)
    
    elif config.strategy == RetryStrategy.EXPONENTIAL:
        delay = config.initial_delay * (config.exponential_base ** attempt)
    
    else:
        delay = config.initial_delay
    
    # Cap at max delay
    return min(delay, config.max_delay)


def retry(config: Optional[RetryConfig] = None):
    """
    Decorator for retrying function calls with configurable backoff.
    
    Args:
        config: Retry configuration
        
    Example:
        @retry(RetryConfig(max_attempts=3, strategy=RetryStrategy.EXPONENTIAL))
        def unreliable_function():
            ...
    """
    if config is None:
        config = RetryConfig()
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(config.max_attempts):
                try:
                    result = func(*args, **kwargs)
                    
                    # Check if result should trigger retry
                    if config.retry_on_result and config.retry_on_result(result):
                        if attempt < config.max_attempts - 1:
                            delay = calculate_retry_delay(attempt, config)
                            logger.warning(
                                f"{func.__name__} returned retry-worthy result, "
                                f"retrying in {delay:.2f}s (attempt {attempt + 1}/{config.max_attempts})"
                            )
                            time.sleep(delay)
                            continue
                    
                    return result
                    
                except config.retry_on_exceptions as e:
                    last_exception = e
                    
                    if attempt < config.max_attempts - 1:
                        delay = calculate_retry_delay(attempt, config)
                        logger.warning(
                            f"{func.__name__} failed with {type(e).__name__}, "
                            f"retrying in {delay:.2f}s (attempt {attempt + 1}/{config.max_attempts}): {e}"
                        )
                        time.sleep(delay)
                    else:
                        logger.error(
                            f"{func.__name__} failed after {config.max_attempts} attempts: {e}"
                        )
            
            # All retries exhausted
            if last_exception:
                raise last_exception
            
            # This shouldn't happen, but just in case
            raise RuntimeError(f"{func.__name__} failed without exception")
        
        return wrapper
    return decorator


def async_retry(config: Optional[RetryConfig] = None):
    """
    Decorator for retrying async function calls with configurable backoff.
    
    Args:
        config: Retry configuration
        
    Example:
        @async_retry(RetryConfig(max_attempts=3))
        async def unreliable_async_function():
            ...
    """
    if config is None:
        config = RetryConfig()
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(config.max_attempts):
                try:
                    result = await func(*args, **kwargs)
                    
                    # Check if result should trigger retry
                    if config.retry_on_result and config.retry_on_result(result):
                        if attempt < config.max_attempts - 1:
                            delay = calculate_retry_delay(attempt, config)
                            logger.warning(
                                f"{func.__name__} returned retry-worthy result, "
                                f"retrying in {delay:.2f}s (attempt {attempt + 1}/{config.max_attempts})"
                            )
                            await asyncio.sleep(delay)
                            continue
                    
                    return result
                    
                except config.retry_on_exceptions as e:
                    last_exception = e
                    
                    if attempt < config.max_attempts - 1:
                        delay = calculate_retry_delay(attempt, config)
                        logger.warning(
                            f"{func.__name__} failed with {type(e).__name__}, "
                            f"retrying in {delay:.2f}s (attempt {attempt + 1}/{config.max_attempts}): {e}"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"{func.__name__} failed after {config.max_attempts} attempts: {e}"
                        )
            
            # All retries exhausted
            if last_exception:
                raise last_exception
            
            raise RuntimeError(f"{func.__name__} failed without exception")
        
        return wrapper
    return decorator


__all__ = [
    'RetryConfig',
    'RetryStrategy',
    'CircuitBreaker',
    'CircuitBreakerState',
    'CircuitBreakerOpenError',
    'retry',
    'async_retry',
    'calculate_retry_delay'
]
```

---
