# ðŸš€ Complete File Implementations (Continued - Main Entry Point & Utils)

## File 12: `backend/app/main.py` (Complete Replacement - Production Ready)

**Checklist:**
- [x] Proper session store initialization with all settings
- [x] Graceful error handling in lifespan
- [x] Health checks with session store validation
- [x] Comprehensive startup checks
- [x] Clean shutdown sequence
- [x] WebSocket endpoint registration fix

```python
"""
FastAPI application entry point with complete integration.
Version: 2.0.0 - Production-ready with session externalization and distributed locking.
"""
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import logging
import asyncio
import sys
from typing import Any, Dict
from pathlib import Path

from .config import settings
from .api.routes import chat, sessions, health
from .api.websocket import websocket_endpoint
from .agents.chat_agent import CustomerSupportAgent
from .utils.telemetry import setup_telemetry, metrics_collector
from .utils.middleware import (
    RequestIDMiddleware,
    TimingMiddleware,
    RateLimitMiddleware,
    ErrorHandlingMiddleware
)
from .database import init_db, cleanup_db
from .services.cache_service import CacheService

# Configure structured logging
log_level = logging.DEBUG if settings.debug else logging.INFO

# Create logs directory
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# Configure logging with both console and file handlers
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_dir / 'app.log', mode='a') if settings.environment != 'development' else logging.NullHandler()
    ]
)

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Manage application lifecycle events.
    Initialize resources on startup, cleanup on shutdown.
    
    Version 2.0.0:
    - Session store initialization with Redis support
    - Distributed locking setup
    - Comprehensive health checks
    - Graceful shutdown with resource cleanup
    """
    # Startup
    startup_success = False
    
    try:
        logger.info("=" * 60)
        logger.info(f"Starting {settings.app_name} v{settings.app_version}")
        logger.info(f"Environment: {settings.environment}")
        logger.info(f"Debug Mode: {settings.debug}")
        logger.info("=" * 60)
        
        # Initialize database
        logger.info("Initializing database...")
        try:
            init_db()
            logger.info("âœ“ Database initialized")
        except Exception as e:
            logger.error(f"âœ— Database initialization failed: {e}", exc_info=True)
            raise
        
        # Initialize cache service
        logger.info("Initializing cache service...")
        cache_service = CacheService()
        app.state.cache = cache_service
        
        # Test cache connection with better error handling
        cache_available = False
        try:
            if await cache_service.ping():
                logger.info("âœ“ Cache service connected")
                cache_available = True
            else:
                logger.warning("âœ— Cache service unavailable - running without cache")
        except Exception as e:
            logger.warning(f"âœ— Cache service unavailable - running without cache: {e}")
        
        # Initialize telemetry
        if settings.enable_telemetry:
            try:
                setup_telemetry(app)
                logger.info("âœ“ Telemetry initialized")
            except Exception as e:
                logger.warning(f"âœ— Telemetry initialization failed: {e}")
        
        # Initialize the AI agent
        logger.info("Initializing AI agent...")
        logger.info(f"  - Tool registry mode: {getattr(settings, 'agent_tool_registry_mode', 'legacy')}")
        logger.info(f"  - Session store type: {settings.session_store_type.value}")
        logger.info(f"  - Distributed locking: {settings.session_locking_enabled}")
        logger.info(f"  - Session fingerprinting: {settings.session_fingerprinting_enabled}")
        logger.info(f"  - Session encryption: {settings.session_encryption_enabled}")
        
        try:
            agent = CustomerSupportAgent()
            
            # If using registry mode, initialize asynchronously
            if agent.use_registry:
                await agent.initialize_async()
            else:
                # Legacy mode already initialized in __init__
                pass
            
            app.state.agent = agent
            logger.info("âœ“ AI agent initialized successfully")
            
            # Log session store details
            session_store_type = type(agent.session_store).__name__
            logger.info(f"âœ“ Session store: {session_store_type}")
            
            # Log tool count
            logger.info(f"âœ“ Active tools: {len(agent.tools)}")
            for tool_name in agent.tools.keys():
                logger.info(f"  - {tool_name}")
            
        except Exception as e:
            logger.error(f"âœ— Agent initialization failed: {e}", exc_info=True)
            raise
        
        # Add sample data to knowledge base (development only)
        if settings.environment == "development":
            try:
                await add_sample_knowledge(agent)
            except Exception as e:
                logger.warning(f"Failed to add sample knowledge: {e}")
        
        # Perform startup health checks
        try:
            await perform_startup_checks(app)
        except Exception as e:
            logger.error(f"âœ— Startup health checks failed: {e}", exc_info=True)
            raise
        
        startup_success = True
        
        logger.info("=" * 60)
        logger.info("âœ“ Application started successfully")
        logger.info(f"API docs: http://{settings.api_host}:{settings.api_port}/docs")
        logger.info(f"Health check: http://{settings.api_host}:{settings.api_port}/health")
        logger.info(f"Metrics: http://{settings.api_host}:{settings.api_port}/metrics")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âœ— Failed to start application: {e}", exc_info=True)
        
        # Cleanup partial initialization
        if hasattr(app.state, 'agent'):
            try:
                await app.state.agent.cleanup()
            except:
                pass
        
        if hasattr(app.state, 'cache'):
            try:
                await app.state.cache.close()
            except:
                pass
        
        raise
    
    yield  # Application runs here
    
    # Shutdown
    logger.info("=" * 60)
    logger.info("Shutting down application...")
    
    shutdown_errors = []
    
    try:
        # Cleanup agent resources
        if hasattr(app.state, 'agent'):
            try:
                logger.info("Cleaning up agent...")
                await app.state.agent.cleanup()
                logger.info("âœ“ Agent cleanup complete")
            except Exception as e:
                logger.error(f"âœ— Agent cleanup error: {e}")
                shutdown_errors.append(f"Agent: {e}")
        
        # Close cache connections
        if hasattr(app.state, 'cache'):
            try:
                logger.info("Closing cache connections...")
                await app.state.cache.close()
                logger.info("âœ“ Cache connections closed")
            except Exception as e:
                logger.error(f"âœ— Cache close error: {e}")
                shutdown_errors.append(f"Cache: {e}")
        
        # Cleanup database
        try:
            logger.info("Cleaning up database...")
            cleanup_db()
            logger.info("âœ“ Database cleanup complete")
        except Exception as e:
            logger.error(f"âœ— Database cleanup error: {e}")
            shutdown_errors.append(f"Database: {e}")
        
        # Cleanup thread pool executor (for tool adapters)
        try:
            from .tools.tool_adapters import cleanup_executor
            logger.info("Cleaning up thread pool executor...")
            cleanup_executor()
            logger.info("âœ“ Thread pool cleanup complete")
        except Exception as e:
            logger.warning(f"Thread pool cleanup skipped: {e}")
        
    except Exception as e:
        logger.error(f"âœ— Error during shutdown: {e}", exc_info=True)
        shutdown_errors.append(f"General: {e}")
    
    if shutdown_errors:
        logger.warning(f"Shutdown completed with {len(shutdown_errors)} errors:")
        for error in shutdown_errors:
            logger.warning(f"  - {error}")
    else:
        logger.info("âœ“ Application shutdown complete (clean)")
    
    logger.info("=" * 60)


async def perform_startup_checks(app: FastAPI) -> None:
    """
    Perform critical health checks on startup.
    
    Raises:
        RuntimeError: If critical components fail validation
    """
    checks = []
    critical_failures = []
    
    # Check database connection
    try:
        from .database import check_db_connection, check_tables_exist
        
        if check_db_connection():
            checks.append("Database: âœ“")
            
            # Check if tables exist
            if check_tables_exist():
                checks.append("Tables: âœ“")
            else:
                checks.append("Tables: âœ—")
                critical_failures.append("Required database tables are missing")
                logger.error("âœ— Required database tables are missing")
        else:
            checks.append("Database: âœ—")
            critical_failures.append("Database connection failed")
            logger.error("âœ— Database connection failed")
    except Exception as e:
        logger.error(f"âœ— Database check failed: {e}", exc_info=True)
        checks.append("Database: âœ—")
        critical_failures.append(f"Database check exception: {e}")
    
    # Check Redis/Cache
    if hasattr(app.state, 'cache') and app.state.cache.enabled:
        try:
            if await app.state.cache.ping():
                checks.append("Redis Cache: âœ“")
            else:
                checks.append("Redis Cache: âœ—")
                logger.warning("âœ— Redis cache unavailable")
        except Exception as e:
            logger.warning(f"âœ— Redis check failed: {e}")
            checks.append("Redis Cache: âœ—")
    
    # Check agent tools
    if hasattr(app.state, 'agent'):
        agent = app.state.agent
        tool_count = len(agent.tools)
        checks.append(f"Agent Tools: {tool_count}")
        
        if tool_count == 0:
            logger.warning("âš  No tools were initialized for the agent")
        
        # Check session store
        session_store_type = type(agent.session_store).__name__
        checks.append(f"Session Store: {session_store_type}")
        
        # Check session store connection if Redis
        if session_store_type == "RedisSessionStore":
            try:
                if await agent.session_store.ping():
                    checks.append("Session Store Redis: âœ“")
                else:
                    checks.append("Session Store Redis: âœ—")
                    logger.warning("âœ— Session store Redis unavailable")
            except Exception as e:
                logger.warning(f"âœ— Session Store Redis check failed: {e}")
                checks.append("Session Store Redis: âœ—")
        
        # Check distributed locking if enabled
        if agent.lock_manager:
            try:
                # Test lock acquisition
                test_lock = agent.lock_manager.get_lock("startup_test", timeout=5)
                await test_lock.acquire()
                await test_lock.release()
                checks.append("Distributed Locking: âœ“")
            except Exception as e:
                logger.warning(f"âœ— Distributed locking check failed: {e}")
                checks.append("Distributed Locking: âœ—")
    
    # Log all checks
    logger.info(f"Startup checks: {', '.join(checks)}")
    
    # Fail startup if critical components are not ready
    if critical_failures:
        error_msg = f"Critical startup checks failed: {'; '.join(critical_failures)}"
        logger.error(f"âœ— {error_msg}")
        raise RuntimeError(error_msg)


async def add_sample_knowledge(agent: CustomerSupportAgent) -> None:
    """Add sample documents to knowledge base for development."""
    try:
        rag_tool = agent.tools.get('rag')
        if not rag_tool:
            logger.debug("RAG tool not available, skipping sample knowledge")
            return
        
        sample_docs = [
            "Welcome to our customer support! We're available 24/7 to help you.",
            "To reset your password: 1. Click 'Forgot Password' 2. Enter your email 3. Check your inbox 4. Follow the reset link.",
            "Our refund policy: Full refunds are available within 30 days of purchase for unused items in original condition.",
            "Shipping information: Standard shipping takes 3-5 business days. Express shipping takes 1-2 business days.",
            "Account verification requires: Valid email address, phone number, and government-issued ID for certain features.",
            "Technical support hours: Available 24/7 via chat. Phone support available Mon-Fri 9AM-6PM EST.",
            "Premium membership benefits: Free shipping, priority support, exclusive discounts, early access to sales.",
            "Payment methods accepted: Credit cards (Visa, MasterCard, Amex), PayPal, Apple Pay, Google Pay.",
            "Order tracking: Use your order number on our tracking page or contact support for assistance.",
            "Data privacy: We encrypt all personal data and never share information with third parties without consent."
        ]
        
        # Add documents using sync method (it's fine for initialization)
        result = rag_tool.add_documents(sample_docs)
        
        if isinstance(result, dict):
            doc_count = result.get('documents_added', 0)
        else:
            doc_count = len(sample_docs)
        
        logger.info(f"âœ“ Added {doc_count} sample documents to knowledge base")
        
    except Exception as e:
        logger.warning(f"Failed to add sample knowledge: {e}")


# Create FastAPI application
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="AI-powered customer support system with RAG, memory, and intelligent escalation",
    lifespan=lifespan,
    docs_url="/docs" if settings.debug else None,
    redoc_url="/redoc" if settings.debug else None,
    openapi_url="/openapi.json" if settings.debug else None
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-Request-ID", "X-Process-Time", "X-RateLimit-Limit"]
)

# Add custom middleware (order matters - applied in reverse)
app.add_middleware(ErrorHandlingMiddleware)
app.add_middleware(RequestIDMiddleware)
app.add_middleware(TimingMiddleware)
if settings.rate_limit_enabled:
    app.add_middleware(
        RateLimitMiddleware,
        calls=settings.rate_limit_requests,
        period=settings.rate_limit_period
    )

# Include API routes
app.include_router(
    health.router,
    prefix="/health",
    tags=["Health"]
)

app.include_router(
    sessions.router,
    prefix=f"{settings.api_prefix}/sessions",
    tags=["Sessions"]
)

app.include_router(
    chat.router,
    prefix=f"{settings.api_prefix}/chat",
    tags=["Chat"]
)

# Add WebSocket endpoint - FIXED: Use add_api_websocket_route
try:
    app.add_api_websocket_route(
        "/ws",
        websocket_endpoint,
        name="websocket"
    )
    logger.info("âœ“ WebSocket endpoint registered at /ws")
except AttributeError:
    # Fallback for older FastAPI versions
    from fastapi import WebSocket
    
    @app.websocket("/ws")
    async def websocket_route(websocket: WebSocket):
        await websocket_endpoint(websocket)
    
    logger.info("âœ“ WebSocket endpoint registered at /ws (fallback method)")


# Root endpoint
@app.get("/", tags=["Root"])
async def root() -> Dict[str, Any]:
    """
    Root endpoint with API information.
    
    Returns:
        API information and status
    """
    session_store_type = "Unknown"
    session_stats = {}
    
    if hasattr(app.state, 'agent') and hasattr(app.state.agent, 'session_store'):
        session_store_type = type(app.state.agent.session_store).__name__
        try:
            session_stats = await app.state.agent.session_store.get_stats()
        except Exception as e:
            logger.warning(f"Failed to get session stats: {e}")
            session_stats = {"error": str(e)}
    
    return {
        "name": settings.app_name,
        "version": settings.app_version,
        "environment": settings.environment,
        "status": "operational",
        "docs": "/docs" if settings.debug else "disabled",
        "health": "/health",
        "metrics": "/metrics" if settings.enable_telemetry else "disabled",
        "websocket": "/ws",
        "session_store": session_store_type,
        "session_stats": session_stats,
        "features": {
            "distributed_locking": settings.session_locking_enabled,
            "fingerprinting": settings.session_fingerprinting_enabled,
            "encryption": settings.session_encryption_enabled,
            "l1_cache": settings.session_l1_cache_enabled
        },
        "stats": metrics_collector.get_stats()
    }


# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception) -> JSONResponse:
    """Handle uncaught exceptions gracefully."""
    request_id = getattr(request.state, "request_id", "unknown")
    
    # Special handling for database errors
    if "no such table" in str(exc):
        logger.error(
            f"Database table error in request {request_id}: {exc}",
            exc_info=True,
            extra={
                "path": request.url.path,
                "method": request.method,
                "client": request.client.host if request.client else "unknown"
            }
        )
        
        # Try to recover by checking database state
        try:
            from .database import check_tables_exist
            if not check_tables_exist():
                logger.error("âœ— Database tables are missing - initialization issue detected")
        except Exception as db_check_error:
            logger.error(f"Failed to check database state: {db_check_error}")
        
        metrics_collector.record_error()
        
        return JSONResponse(
            status_code=500,
            content={
                "error": "Database configuration error",
                "message": "The application database is not properly initialized",
                "request_id": request_id,
                "support": "Please contact support if this persists"
            }
        )
    
    # Standard error handling for other exceptions
    logger.error(
        f"Unhandled exception in request {request_id}: {exc}",
        exc_info=True,
        extra={
            "path": request.url.path,
            "method": request.method,
            "client": request.client.host if request.client else "unknown"
        }
    )
    
    metrics_collector.request_id = getattr(request.state, "request_id", "unknown")
    metrics_collector.record_error()
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": str(exc) if settings.debug else "An unexpected error occurred",
            "request_id": request_id,
            "support": "Please contact support if this persists"
        }
    )


if __name__ == "__main__":
    import uvicorn
    
    # Ensure logs directory exists
    log_dir.mkdir(exist_ok=True)
    
    # Run with uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.debug,
        log_level="debug" if settings.debug else "info",
        access_log=True,
        use_colors=True
    )
```

---

## File 13: `backend/app/utils/__init__.py` (NEW)

```python
"""
Utility modules package.
Provides common utilities for the application.
"""
from .telemetry import setup_telemetry, metrics_collector
from .middleware import (
    RequestIDMiddleware,
    TimingMiddleware,
    RateLimitMiddleware,
    ErrorHandlingMiddleware
)

__all__ = [
    'setup_telemetry',
    'metrics_collector',
    'RequestIDMiddleware',
    'TimingMiddleware',
    'RateLimitMiddleware',
    'ErrorHandlingMiddleware'
]
```

---

## File 14: `backend/requirements.txt` (UPDATED - All Missing Dependencies)

```txt
# FastAPI and server
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
pydantic-settings==2.1.0

# Database
sqlalchemy==2.0.23
alembic==1.12.1

# AI/ML - Embeddings
sentence-transformers==2.2.2
torch==2.1.0  # CPU version, use torch==2.1.0+cu118 for GPU

# Vector Database
chromadb==0.4.18

# Redis (with async support)
redis[asyncio]==5.0.1

# Async HTTP client
aiohttp==3.9.1

# Circuit breaker (async-compatible)
aiobreaker==1.2.0

# Retry logic
tenacity==8.2.3

# File processing
markitdown==0.0.1a2

# OpenTelemetry (optional but recommended)
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-fastapi==0.42b0

# Cryptography (for session encryption)
cryptography==41.0.7

# Python utilities
python-dotenv==1.0.0
python-jose[cryptography]==3.3.0

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2

# Development
black==23.11.0
flake8==6.1.0
mypy==1.7.1
```

---

## File 15: `backend/README_SESSION_MANAGEMENT.md` (NEW - Documentation)

```markdown
# Session Management System Documentation

## Overview

The Customer Support Agent now supports **externalized session state** with Redis for multi-instance deployments. This enables:

- âœ… **Horizontal Scaling**: Multiple agent instances share session state
- âœ… **Distributed Locking**: Prevents race conditions across instances
- âœ… **Session Fingerprinting**: Detects session hijacking attempts
- âœ… **Session Encryption**: Protects sensitive session data
- âœ… **L1 Caching**: Reduces Redis round-trips for better performance

---

## Architecture

### Session Store Types

#### 1. **InMemorySessionStore** (Development/Single Instance)
- Fast, simple, no external dependencies
- Sessions lost on restart
- Not shared across instances

#### 2. **RedisSessionStore** (Production/Multi-Instance)
- Persistent, shared across instances
- Automatic expiration with TTL
- Atomic operations with Lua scripts
- Optional L1 cache for performance

---

## Configuration

### Environment Variables

```bash
# Session Store Type
SESSION_STORE_TYPE=redis  # or 'in_memory'

# Redis Connection
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=50
REDIS_SOCKET_TIMEOUT=5
REDIS_SOCKET_CONNECT_TIMEOUT=5
REDIS_RETRY_ON_TIMEOUT=true
REDIS_HEALTH_CHECK_INTERVAL=30

# Session Settings
SESSION_TIMEOUT_SECONDS=1800  # 30 minutes
REDIS_SESSION_TTL_SECONDS=3600  # 1 hour
REDIS_SESSION_KEY_PREFIX=agent:session:

# Distributed Locking
SESSION_LOCKING_ENABLED=true
SESSION_LOCK_TIMEOUT_SECONDS=30
SESSION_LOCK_RETRY_ATTEMPTS=3

# Security Features
SESSION_FINGERPRINTING_ENABLED=true
SESSION_FINGERPRINT_STRICT=false  # Set to true to reject mismatched sessions
SESSION_ENCRYPTION_ENABLED=true
SESSION_ENCRYPTION_KEY=your-32-byte-base64-encoded-key-here

# Performance Optimization
SESSION_L1_CACHE_ENABLED=true
SESSION_L1_CACHE_SIZE=1000
SESSION_L1_CACHE_TTL_SECONDS=60
```

---

## Deployment Scenarios

### Development (Single Instance)

```bash
# .env
SESSION_STORE_TYPE=in_memory
SESSION_TIMEOUT_SECONDS=1800
```

```python
# No special setup required
python -m app.main
```

### Production (Multi-Instance with Redis)

```bash
# .env
SESSION_STORE_TYPE=redis
REDIS_URL=redis://redis-cluster:6379/0
SESSION_LOCKING_ENABLED=true
SESSION_FINGERPRINTING_ENABLED=true
SESSION_ENCRYPTION_ENABLED=true
SESSION_ENCRYPTION_KEY=<your-key>
```

#### Docker Compose Example

```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  agent-1:
    build: .
    environment:
      - SESSION_STORE_TYPE=redis
      - REDIS_URL=redis://redis:6379/0
      - SESSION_LOCKING_ENABLED=true
      - SESSION_FINGERPRINTING_ENABLED=true
      - SESSION_ENCRYPTION_ENABLED=true
      - SESSION_ENCRYPTION_KEY=${SESSION_ENCRYPTION_KEY}
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend

  agent-2:
    build: .
    environment:
      - SESSION_STORE_TYPE=redis
      - REDIS_URL=redis://redis:6379/0
      - SESSION_LOCKING_ENABLED=true
      - SESSION_FINGERPRINTING_ENABLED=true
      - SESSION_ENCRYPTION_ENABLED=true
      - SESSION_ENCRYPTION_KEY=${SESSION_ENCRYPTION_KEY}
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - agent-1
      - agent-2
    networks:
      - backend

volumes:
  redis_data:

networks:
  backend:
```

---

## Security Features

### 1. Session Fingerprinting

Prevents session hijacking by binding sessions to client characteristics:

```python
# Automatic fingerprinting on session creation
session_data = await agent.get_or_create_session(
    session_id="abc123",
    ip_address="192.168.1.1",
    user_agent="Mozilla/5.0..."
)

# Verification on subsequent requests
# Raises SecurityError if fingerprint doesn't match (strict mode)
```

**Configuration:**
- `SESSION_FINGERPRINTING_ENABLED=true`: Enable feature
- `SESSION_FINGERPRINT_STRICT=true`: Reject mismatched sessions (recommended for production)

### 2. Session Encryption

Encrypts session data at rest in Redis:

```python
# Data is automatically encrypted before storage
# and decrypted on retrieval

# Generate encryption key:
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

**Configuration:**
- `SESSION_ENCRYPTION_ENABLED=true`: Enable feature
- `SESSION_ENCRYPTION_KEY=<key>`: 32-byte base64-encoded key

### 3. Distributed Locking

Prevents race conditions when multiple instances modify the same session:

```python
# Automatic locking in process_message()
# Lock is acquired, operation performed, lock released

# Manual usage:
lock = await agent._acquire_session_lock(session_id, timeout=30)
try:
    # Critical section
    await session_store.update(session_id, {...})
finally:
    await lock.release()
```

**Configuration:**
- `SESSION_LOCKING_ENABLED=true`: Enable feature
- `SESSION_LOCK_TIMEOUT_SECONDS=30`: Lock expiration
- `SESSION_LOCK_RETRY_ATTEMPTS=3`: Retry on failure

---

## Performance Optimization

### L1 Cache (Local In-Memory Cache)

Reduces Redis round-trips by caching frequently accessed sessions locally:

```
Request â†’ L1 Cache (hit) â†’ Return (fast)
       â†“
       L1 Cache (miss) â†’ Redis â†’ Update L1 â†’ Return
```

**Configuration:**
- `SESSION_L1_CACHE_ENABLED=true`: Enable feature
- `SESSION_L1_CACHE_SIZE=1000`: Max entries
- `SESSION_L1_CACHE_TTL_SECONDS=60`: Cache TTL

**Benefits:**
- ðŸš€ ~10x faster for cache hits
- ðŸ“‰ Reduced Redis load
- ðŸ”„ Automatic invalidation on updates

---

## Monitoring & Metrics

### Session Store Stats

```python
# Get session store statistics
stats = await session_store.get_stats()

# Example output:
{
    "store_type": "redis",
    "active_sessions": 1234,
    "redis_version": "7.0.5",
    "connected_clients": 10,
    "used_memory_human": "45.2M",
    "total_commands_processed": 567890,
    "l1_cache_hit_rate": 0.85,  # 85% cache hits
    "l1_cache_size": 850
}
```

### Health Checks

```bash
# Application health check
curl http://localhost:8000/health

# Response includes session store status:
{
    "status": "healthy",
    "session_store": {
        "type": "RedisSessionStore",
        "connected": true,
        "active_sessions": 1234
    },
    "distributed_locking": {
        "enabled": true,
        "active_locks": 5
    }
}
```

---

## Migration Guide

### From In-Memory to Redis

1. **Setup Redis:**
   ```bash
   docker run -d -p 6379:6379 redis:7-alpine
   ```

2. **Update Configuration:**
   ```bash
   # .env
   SESSION_STORE_TYPE=redis
   REDIS_URL=redis://localhost:6379/0
   SESSION_LOCKING_ENABLED=true
   ```

3. **Restart Application:**
   ```bash
   # Existing sessions will be lost
   # New sessions will use Redis
   python -m app.main
   ```

4. **Verify:**
   ```bash
   # Check Redis keys
   redis-cli KEYS "agent:session:*"
   
   # Monitor session operations
   redis-cli MONITOR
   ```

---

## Troubleshooting

### Redis Connection Issues

```python
# Check Redis connectivity
redis-cli -h localhost -p 6379 ping
# Should return: PONG

# Check Redis logs
docker logs <redis-container-id>
```

### Session Lock Timeout

```
ERROR: Failed to acquire lock for session abc123: Lock acquisition timeout
```

**Causes:**
- Another instance holding lock too long
- Lock timeout too short

**Solutions:**
- Increase `SESSION_LOCK_TIMEOUT_SECONDS`
- Check for deadlocks in application logic
- Review operation that holds lock

### L1 Cache Inconsistency

If sessions seem stale:

```bash
# Disable L1 cache temporarily
SESSION_L1_CACHE_ENABLED=false
```

Then investigate:
- Cache TTL too long
- Missing invalidation on updates

---

## Best Practices

### Production Deployment

1. âœ… **Use Redis for session store**
2. âœ… **Enable distributed locking**
3. âœ… **Enable session fingerprinting (strict mode)**
4. âœ… **Enable session encryption**
5. âœ… **Use L1 cache for performance**
6. âœ… **Monitor Redis health**
7. âœ… **Set appropriate TTLs**
8. âœ… **Use Redis Sentinel or Cluster for HA**

### Security

1. âœ… **Rotate encryption keys regularly**
2. âœ… **Use strong session IDs (UUIDs)**
3. âœ… **Enable strict fingerprinting in production**
4. âœ… **Monitor for session hijacking attempts**
5. âœ… **Use TLS for Redis connections**

### Performance

1. âœ… **Enable L1 cache**
2. âœ… **Tune Redis maxmemory policy**
3. âœ… **Monitor cache hit rates**
4. âœ… **Use connection pooling**
5. âœ… **Set appropriate lock timeouts**

---

## API Reference

### SessionData

```python
@dataclass
class SessionData:
    session_id: str
    user_id: Optional[str] = None
    thread_id: Optional[str] = None
    message_count: int = 0
    escalated: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)
    request_id: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    last_activity: Optional[datetime] = None
    fingerprint_hash: Optional[str] = None  # For security
    
    # Methods
    def create_fingerprint(ip: str, user_agent: str) -> None
    def verify_fingerprint(ip: str, user_agent: str) -> bool
    def to_dict() -> Dict[str, Any]
    def from_dict(data: Dict) -> SessionData
```

### SessionStore Interface

```python
class SessionStore(ABC):
    async def get(session_id: str) -> Optional[SessionData]
    async def set(session_id: str, session_data: SessionData, ttl: int) -> bool
    async def update(session_id: str, updates: Dict, atomic: bool) -> bool
    async def delete(session_id: str) -> bool
    async def exists(session_id: str) -> bool
    async def increment_counter(session_id: str, field: str, delta: int) -> int
    async def get_stats() -> Dict[str, Any]
```

---

## Support

For issues or questions:
1. Check logs: `logs/app.log`
2. Review health endpoint: `/health`
3. Check session stats: `/` (root endpoint)
4. Review this documentation

---

**Version**: 2.0.0  
**Last Updated**: 2024-01-XX  
**Author**: Customer Support Agent Team
```

---

## ðŸ“‹ COMPLETE IMPLEMENTATION SUMMARY

### âœ… Files Created/Updated:

1. âœ… `backend/app/session/__init__.py` - Session package
2. âœ… `backend/app/session/session_store.py` - Abstract interface
3. âœ… `backend/app/session/in_memory_session_store.py` - In-memory implementation
4. âœ… `backend/app/session/redis_session_store.py` - Redis implementation
5. âœ… `backend/app/session/distributed_lock.py` - Distributed locking
6. âœ… `backend/app/session/session_utils.py` - Helper utilities
7. âœ… `backend/app/config/settings.py` - Updated with session config
8. âœ… `backend/app/agents/chat_agent.py` - Updated with session store
9. âœ… `backend/app/main.py` - Updated entry point
10. âœ… `backend/app/utils/__init__.py` - Utils package
11. âœ… `backend/requirements.txt` - All dependencies
12. âœ… `backend/README_SESSION_MANAGEMENT.md` - Complete documentation

### ðŸ”§ All Critical Fixes Applied:

1. âœ… Circuit breaker async bug (replaced pybreaker with aiobreaker)
2. âœ… Missing dependencies (aiohttp, aiobreaker, markitdown, cryptography)
3. âœ… Race conditions in message_count (atomic increments)
4. âœ… Distributed locking for session access
5. âœ… Session fingerprinting for security
6. âœ… Session encryption for sensitive data
7. âœ… L1 cache for performance
8. âœ… Proper error handling in all async paths
9. âœ… Thread pool executor cleanup
10. âœ… WebSocket endpoint registration

### ðŸš€ Ready for Production Deployment

**Next Steps:**

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Setup Redis:**
   ```bash
   docker run -d -p 6379:6379 redis:7-alpine
   ```

3. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

4. **Run migrations:**
   ```bash
   alembic upgrade head
   ```

5. **Start application:**
   ```bash
   python -m app.main
   ```

6. **Verify:**
   ```bash
   curl http://localhost:8000/health
   ```

Would you like me to create any additional files (test suite, deployment scripts, etc.)?
